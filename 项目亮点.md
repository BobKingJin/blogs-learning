1、先扣减redis缓存中的数据，再发送mq异步扣减数据库中的数据


异步扣减库存存在的问题

1. 如果发送消息失败，只能回滚Redis。
2. 消费端从数据库扣减操作执行失败，如何处理（这里默认会成功）？
3. 下单失败无法正确回补库存（比如用户取消订单）。

所以需要引入**事务型消息**


2、异步消息发送时机问题

目前扣减库存的事务`ItemService.decreaseStock`是封装在`OrderService.createOrder`事务里面的
在扣减Redis库存、发送异步消息之后，还有订单入库、增加销量的操作。如果这些操作失败，那么`createOrder`**事务会回滚**，`decreaseStock`**事务也回滚**，
但是Redis的**扣减操作却不能回滚**，会导致数据不一致

### 解决方法

解决的方法就是在订单入库、增加销量成功之后，再发送异步消息，`ItemService.decreaseStock`只**负责扣减Redis库存**，**不发送异步消息**


## 事务提交问题

Spring的`@Transactional`标签，会在**事务方法返回后才提交**，如果提交的过程中，发生了异常，则数据库回滚，但是Redis库存已扣，还是无法保证一致性。需要在**事务提交成功后**，**再发送异步消息**。

### 解决方法

Spring提供了`TransactionSynchronizationManager.registerSynchronization`方法，这个方法的传入一个`TransactionSynchronizationAdapter`的匿名类，通过`afterCommit`方法，在**事务提交成功后**，执行**发送消息操作**。


## 事务型消息
假设现在**事务提交成功了**，等着执行`afterCommit`方法，这个时候**突然宕机了**，那么**订单已然入库**，**销量已然增加**，但是**去数据库扣减库存的这条消息**却“**丢失**”了。这里就需要引入RocketMQ的事务型消息。

所谓事务型消息，也会被发送到消息队列里面，这条消息处于`prepared`状态，`broker`会接受到这条消息，**但是不会把这条消息给消费者消费**。

处于`prepared`状态的消息，会执行`TransactionListener`的`executeLocalTransaction`方法，根据执行结果，**改变事务型消息的状态**，**让消费端消费或是不消费**。

//注意，发送的是sendMessageInTransaction
transactionMQProducer.sendMessageInTransaction(message, argsMap);
就会发送一个事务型消息到`broke`，而处于`prepared`状态的事务型消息，会执行`TransactionListener`的`executeLocalTransaction`方法：

transactionMQProducer.setTransactionListener(new TransactionListener() {
    @Override
    public LocalTransactionState executeLocalTransaction(Message message, Object args) {
    //在事务型消息中去进行下单
    Integer itemId = (Integer) ((Map) args).get("itemId");
    Integer promoId = (Integer) ((Map) args).get("promoId");
    Integer userId = (Integer) ((Map) args).get("userId");
    Integer amount = (Integer) ((Map) args).get("amount");
    try {
        //调用下单接口
        orderService.createOrder(userId, itemId, promoId, amount);
    } catch (BizException e) {
        e.printStackTrace();
        //发生异常就回滚消息
        return LocalTransactionState.ROLLBACK_MESSAGE;
    }
    return LocalTransactionState.COMMIT_MESSAGE;
}

这样，在**事务型消息中去执行下单操作**，下单失败，则消息回滚，**不会去数据库扣减库存**。下单成功，则消息被消费，**扣减数据库库存**。



1、秒杀活动

1.1、Spring Boot内嵌Tomcat默认的线程设置——默认**最大等待队列**为100，默认**最大可连接**数为10000，默认**最大工作线程**数为200，默认**最小工作线程**数为10。对于最大连接数，一般默认的10000就行了

重新设置为**最大等待队列**设为1000，**最大工作线程**数设为800，**最小工作线程**数设为100
压测1000*40个请求，发现TPS峰值高达**4100**多，平均响应时间在**145**毫秒左右


Spring Boot**内嵌Tomcat网络连接优化**
一些配置需要通过` WebServerFactoryCustomizer<ConfigurableWebServerFactory>`接口来实现自定义，这里需要自定义`KeepAlive`长连接的配置，减少客户端和服务器的连接请求次数，避免重复建立连接，提高性能


1.2、定时任务提前将**商品详情**缓存加载到本地缓存和redis中，提前将**库存信息**加载到redis中

1.3、提前预测，例如对买家每天访问的商品进行大数据计算，然后统计出TOP N的商品，可以认为这些TOP N的商品就是热点商品

1.4、动态热点发现系统的具体实现:
1. 构建一个异步的系统，它可以收集交易链路上各个环节中的中间件产品的热点Key，如Nginx、缓存、RPC服务框架等这些中间件（一些中间件产品本身已经有热点统计模块）
2. 建立一个热点上报和可以按照需求订阅的热点服务的下发规范，主要目的是通过交易链路上各个系统（包括详情、购物车、交易、优惠、库存、物流等）访问的时间差，把上游已经发现的热点透传给下游系统，提前做好保护。比如，对于大促高峰期，详情系统是最早知道的，在统一接入层上Nginx模块统计的热点URL
3. 将上游系统收集的热点数据发送到热点服务台，然后下游系统（如交易系统）就会知道哪些商品会被频繁调用，然后做热点保护


通过部署在每台机器上的Agent把日志汇总到聚合和分析集群中，然后把符合一定规则的热点数据，通过订阅分发系统再推送到相应的系统中。可以是把热点数据填充到Cache中，或者直接推送到应用服务器的内存中，还可以对这些数据进行拦截，总之下游系统可以订阅这些数据，然后根据自己的需求决定如何处理这些数据

打造热点发现系统时，根据以往经验总结了几点注意事项

1. 这个热点服务后台抓取热点数据日志最好采用异步方式，因为“异步”一方面便于保证通用性，另一方面又不影响业务系统和中间件产品的主流程
2. 热点服务发现和中间件自身的热点保护模块并存，每个中间件和应用还需要保护自己。热点服务台提供热点数据的收集和订阅服务，便于把各个系统的热点数据透明出来
3. 热点发现要做到接近实时（3s内完成热点数据的发现），因为只有做到接近实时，动态发现才有意义，才能实时地对下游系统提供保护

1.5、如果秒杀商品的减库存逻辑非常单一，比如没有复杂的SKU库存和总库存这种联动关系的话，完全可以直接在redis中扣减库存

1.6、单个热点商品会影响整个数据库的性能， 导致0.01%的商品影响99.99%的商品的售卖，这是不愿意看到的情况。一个解决思路是遵循前面介绍的原则进行隔离，把热点商品放到单独的热点库中。但是这无疑会带来维护上的麻烦，比如要做热点数据的动态迁移以及单独的数据库等

当把**库存存到Redis的时候**，**商品可能被下单**，这样数据库的库存和Redis的库存就**不一致**了
解决方法就是活动**未开始**的时候，商品是**下架状态**，不能被下单

1.7、利用多线程 发送**mq事务型消息**: 创建订单，扣减redis缓存，增加销量

1.7.1、异步扣减库存存在的问题
1. 如果发送消息失败，mysql中创建订单，增加销量等操作无法回滚，只能回滚Redis
2. 消费端从数据库扣减操作执行失败，如何处理（这里默认会成功）
3. 下单失败无法正确回补库存（比如用户取消订单）

在扣减Redis库存、发送异步消息之后，还有订单入库、增加销量的操作。如果这些操作失败，那么`createOrder`**事务会回滚**，`decreaseStock`**事务也回滚**，但是Redis的**扣减操作却不能回滚**，会导致数据不一致

解决的方法就是在订单入库、增加销量成功之后，再发送异步消息，`ItemService.decreaseStock`只**负责扣减Redis库存**，**不发送异步消息**

但是这么做，依然有问题。Spring的`@Transactional`标签，会在**事务方法返回后才提交**，如果提交的过程中，发生了异常，则数据库回滚，但是Redis库存已扣，还是无法保证一致性。需要在**事务提交成功后**，**再发送异步消息**

Spring提供了`TransactionSynchronizationManager.registerSynchronization`方法，这个方法的传入一个`TransactionSynchronizationAdapter`的匿名类，通过`afterCommit`方法，在**事务提交成功后**，执行**发送消息操作**

假设现在**事务提交成功了**，等着执行`afterCommit`方法，这个时候**突然宕机了**，那么**订单已然入库**，**销量已然增加**，但是**去数据库扣减库存的这条消息**却“**丢失**”了，需要引入RocketMQ的**事务型消息**

当执行`orderService.createOrder`后，突然**又宕机了**，根本没有返回，这个时候事务型消息就会进入`UNKNOWN`状态

在匿名类`TransactionListener`里面，还需要覆写`checkLocalTransaction`方法，这个方法就是用来处理`UNKNOWN`状态的。应该怎么处理？这就需要引入**库存流水**

1、用户请求后端`OrderController.createOrder`接口，先初始化库存流水的状态，再调用事务型消息去下单
2、事务型消息会调用`OrderService.createOrder`方法，执行Redis扣减库存、订单入库、销量增加的操作，当这些操作都完成后，就说明下单完成了，**等着异步更新数据库了**，那么需要修改订单流水的状态
3、异步更新数据库，需要事务型消息从`prepare`状态变成`commit`状态。假如此时`orderService.createOrder`**本身发生了异常**，那么就回滚事务型消息，并且返回`LocalTransactionState.ROLLBACK_MESSAGE`，这个下单操作就会被取消
如果**本身没有发生异常**，那么就返回`LocalTransactionState.COMMIT_MESSAGE`，此时事务型消息会从`prepare`状态变为`commit`状态，接着被消费端消费，异步扣减库存
4、如果在执行`createOrder`的时候，突然宕机了，此时事务型消息的状态是`UNKNOWN`，需要在`TransactionListener.checkLocalTransaction`方法中进行处理

1.8、如果库存没有了，就打上**售罄标志**，尽量将请求拦截在前面

1.9、对接风控系统，并在用户维度进行限流限制

1.10、从接口层面和数据库层面，两个维度进行限流，进行单机限流

1.11、隐藏秒杀接口：如果秒杀地址直接暴露，在秒杀开始前可能会被恶意用户来刷接口，因此需要在没到秒杀开始时间不能获取秒杀接口，只有秒杀开始了，才返回秒杀地址 url 和验证 MD5，用户拿到这两个数据才可以进行秒杀

1.12、lua脚本扣减库存
StringBuilder lua = new StringBuilder();  
lua.append("if (redis.call('exists', KEYS[1]) == 1) then");  
lua.append("  local stock = tonumber(redis.call('get', KEYS[1]));");  
lua.append("  if (stock == -1) then");  
lua.append("    return 1;");  
lua.append("  end;");  
lua.append("  if (stock > 0) then");  
lua.append("    redis.call('incrby', KEYS[1], -1);");  
lua.append("    return stock;");  
lua.append("  end;");  
lua.append("  return 0;");  
lua.append("end;");  
lua.append("return -1;");
该代码的主要流程如下：
先判断商品id是否存在，如果不存在则直接返回。
获取该商品id的库存，判断库存如果是-1，则直接返回，表示不限制库存。
如果库存大于0，则扣减库存。
如果库存等于0，是直接返回，表示库存不足。


内存压缩优化

1、数据推送只能实时查询，不能用hive尽心数据归档或者收容

一个简单的实验结果，统计了在开启指针压缩的64位机器上，不同数据条数的键值类型均为Integer的HashMap的内存占存。作为参考，将其所存储的所有Integer实例的占存视作数据占存，将剩余的占存视作结构占存。从下表的实验结果来看，无论在何种数据规模下，HashMap内部结构的内存开销占比都很高，占到了整体的55%以上

HashMap主要由一个哈希桶数组及多个存储在哈希桶中的节点Node<Key,Value>所构成，分别具体解析一下哈希桶数组table和数据节点Node的内存开销

在内存结构上，哈希桶就是由一个附带数组长度的对象头和数组元素集合组成。因此，一个长度为N的哈希桶数组的占存大小就会是：
8（对象头标识位）+ 4（类型指针）+ 4（数组长度 + 4 （实体引用）N （实体数量）字节 + 对齐字节。即，长度为32的哈希桶数组则实际占存即为16 + 4 *32 = 144字节
为了提升读写性能，HashMap中哈希桶数组的实际长度并不会总是等于实际存储的数据量

综合上述的哈希桶长度策略，可以很明显的看到HashMap所存储的哈希桶实体数组在绝大部分情况下总是会将冗余出比实际数据量多一些的空间，以减少哈希碰撞、提升读取效率

所以，在原生HashMap消耗了大量的额外空间结构换取其读写性能。这使得原生HashMap在大数据量且内存有限的应用上，并不是一个最优的缓存结构选型

由于Integer是包装类，因此数组中存储的实际是4字节长的Integer的引用。而对于一个Integer实例本身来说，除了4字节的实际数据外，其还需要12字节来保存其对象头。所以在集合中要保存一个Integer的实际开销就会是4 + 12 + 4 = 20字节

基础类型的int[]则简单的多：在创建数组时，仅需为每个元素开辟4字节来保存整型即可

所以理论上每个Integer都会比int额外产生16字节的内存开销。从实验结果可以看出，若可以直接使用基础类型来代替包装类存储时，可以大幅减少内存占存。此结论对其他如HashMap等数据结构也同样有效


为了在保证读写性能的情况下尽可能压缩内存开销，调研了一些第三方的开源集合框架来尝试在内存和性能上尽可能取得平衡

ConcurrentHashMap、SparseArray、Guava Cache、FastUtil

1.1、位图（BitMap）是一种常见的编码格式，JDK中提供的默认实现为BitSet类。它是用Bit位来存储数据的某种状态，通常指示是非有无。在最常见的情况下，当需要存储大量连续ID是否为True时，用到此类结构就可以大量减少内存的开销。

需要存储的数据的Key为整型， Value为该Key是否有效的状态数据。若是直接存储，则一条数据至少需要4个字节用于存储整型Key以及4个字节用于存储布尔型的状态值。那么，当需要存储Key从1至8的8条数据，则至少需要64字节。若使用位图编码技术对数据进行处理，那么仅需要1个bit即可存储一个True or False的状态信息。因此，就可以使用1个字节存储下所有8条数据的状态信息。此时，该字节的第1位的bit用以表示Key=1的状态信息，第2位的bit用以Key=2的状态信息，以此类推

1.2 游程编码
游程编码是一种无损压缩数据的编码方式，主要方法是使用当前数据元素以及该元素连续出现的次数来取代数据中连续出现的部分。若数据存在大量的数据连续且重复的情况，就可以考虑使用RLE以降低内存。比如，一个内部存储了4个连续的a与6个连续的b的字符串经过游程编码后为a4b6。那么，该字符串长度就从10字节减少至4字节

1.3 字典编码
字典编码是把整体重复性高的数据进行去重后建立字典，把原来存放数据的地方变为指向实体字典引用的编码方式。因为引用指针依然占存，因此适合单个的实例数据字段较多的数据缓存。

1.4 差值编码
差值编码是对于非连续的数据Key通过差值计算的方式转化为连续的Key，让字典可以转化为数组的编码方式。


2.1、将司机相关(例如属于哪些服务商，)数据实体上包括布尔型、枚举以及部分字符串等所有可以枚举的字段进行了位图编码，大幅降低了单个实体的占存大小

2.2、使用字典编码对重复实体进行压缩，大量**重复数据**，在实际处理过程中，会先将房型数据实体进行序列化后转换为MD5，在字典中只存储MD5编码，而实体字典中存储MD5到房型信息实体的关系。在进行数据查询时，则是先通过ID在房型字典中查找到对应的MD5值，然后在实体字典中通过MD5值查找到对应的基础信息实体




3、服务毛刺优化
**计算密集型服务**，司机权益/联合活动 需要进行大量的订单匹配，然后进行奖励


1、有数据和服务之前的依赖关系，必须有序处理有依赖关系，必须保持顺序处理，所以只能串行执行，导致并发处理能力只有1

这个方案解决的不是，某种某个请求的自身的耗时问题，而是解决因为别的请求慢，导致自身也慢的问题

优化: 可以对耗时长的请求，分到专门的集群，可以进行接口处理速度统计，做个标记，针对耗时的任务，专门打到一台机器上面去

2、worker是多台机器进行任务处理，哪个worker空闲，哪个worker去跑任务然后就可以返回



4、Dubbo连接超时问题的排查  https://mp.weixin.qq.com/s/oKtu3AA9D3y--xMDQ8EARw

高并发场景导致的connect timeout

场景: 多调用方，服务每次发布总是会有部分客户端报connect timeout，触发大面积的应用报警。

4.1、服务的端口是否异步打开
调用方的每台机器都要跟160个服务端实例建立连接，但是客户端看到的报错量只有几个。所以，最开始怀疑客户端的连接发到服务端，但是端口没有来得及打开，导致少量的连接失败了。Dubbo代理是在注册之前，而且是同步启动的，这样的话就否定了端口没打开的可能。

4.2、怀疑注册中心推送出现了问题
正常情况下的注册发现机制是在服务端健康检查通过后，再把实例推送到客户端。是否注册中心推送出了问题，服务没注册完就把实例推送到客户端了？或者，客户端实例缓存出现问题导致的呢？

4.3、怀疑端口打开后又被莫名其妙的关闭
不确定是否服务启动后，会有某些未知的场景触发端口被莫名其妙的关闭。于是，在本地模拟服务启动，启动过程中通过shell脚本不停的打印端口的状态。

排查步骤:
Dubbo已经打印了前面看到的端口打开的日志，需要再能够看到服务端连接被accept，首先摘掉服务的流量，然后在Tomcat重启的过程中抓TCPdump。从TCP dump的结果中可以看到，服务端有一阵子收到了TCP的syn，但是全部没有回ack。可是HTTP的syn却正常的回复了syn+ack，难道是应用层把syn包给丢了？


使用缓存的正确姿势: https://juejin.cn/post/6844903604998914055#heading-11

5、vivo全球商城：库存系统架构设计与实践   https://zhuanlan.zhihu.com/p/612667913

可售库存在商品系统、活动库存在营销系统，库存流转也只有扣减与释放，无法针对库存进行整合与业务创新，存在诸多限制

库存系统提供两个核心能力：交易能力和库存管理。上层业务方可以调用提供的API完成库存查询、库存扣减等操作

库存系统一共包含4类库存：可售库存、实物库存、预占库存、活动库存
可售库存：运营配置的普通商品库存，商品维度到SKU。
实物库存：由仓储系统同步到库存系统的实物库存，细化到具体仓库。
预占库存：用户下单完成库存预占，仓储系统发货后释放预占库存，预占库存可以监控已下单未发货库存量。
活动库存：用于秒杀、抢购等各类营销活动的商品库存。

5.1 库存扣减防重
保证库存扣减接口的幂等性。注：保证接口幂等的方案有很多，比如每次扣减库存时，带上唯一的流水号，利用数据库的唯一索引保证幂等等
在库存扣减接口入参中增加订单序列号作为唯一标识，库存扣减时增加一条扣减日志。当接口重复请求时，会优先校验是否已经存在扣减记录，如果已存在则直接返回，避免重复扣减问题
针对高并发库存扣减，比如秒杀，一般采用的是缓存扣减库存的方式（redis+lua脚本实现单线程库存更新）作为前置流程，代替数据库直接更新。


6、vivo 全球商城：优惠券系统架构设计与实践   https://zhuanlan.zhihu.com/p/397770795

用户领券，随机选取库存记录表中已分配的某一库存字段（共M个）进行更新，更新成功即为库存扣减成功

一键领取多张券: 用户优惠券数据做了分库分表，同一用户的优惠券资产保存在同一库表中，因此同一用户可实现批量入库。

保证高并发情况下，用户不会超领: 
假如用户在商城发起请求，一键领取A/B/C/D四张券，同时活动系统给用户发放券A，这两个领券请求是同时的。其中，券A限制了每个用户只能领取一张。按照前述采用分布式锁保证校验的准确性，两次请求的分布式锁的key分别为：

用户id+A_id+B_id+C_id+D_id
用户id+A_id
这种情况下，两次请求的分布式锁并没有发挥作用，因为锁key是不同，数量校验依旧存在错误的可能性。为避免批量领券过程中用户超领现象的发生，在批量领券过程中，对分布锁的获取进行了改造。上例一键领取A/B/C/D四张券，需要批量获取4个分布式锁，锁key为：

用户id+A_id
用户id+B_id
用户id+C_id
用户id+D_id
获取其中任何一个锁失败，即表明此时该用户正在领取其中某一张券，需要自旋等待（在超时时间内）。获取所有的分布式锁成功，才可以进行下一步


定向发券不同于用户主动领券，定向发券的量通常会很大（亿级）。为了支撑大批量的定向发券，定向发券做了一些优化：

1）去除事务。事务逻辑过重，对于定向发券来说没必要。发券失败，记录失败的券，保证失败可以重试。

2）轻量化校验。定向发券限制了券类型，通过限制配置的方式规避需严格校验属性的配置。不同于用户主动领券校验逻辑的冗长，定向发券的校验非常轻量，大大提升发券性能。

3）批量插入。批量券插入减少数据库IO次数，消除数据库瓶颈，提升发券速度。定向发券是针对不同的用户，用户优惠券做了分库分表，为了实现批量插入，需要在内存中先计算出不同用户对应的库表后缀，数据归集后再批量插入，最多插入M次，M为库表总个数。

4）核心参数可动态配置。比如单次发券数量，单次读库数量，发给消息中心的消息体包含的用户数量等，可以控制定向发券的峰值速度和平均速度。

依赖外部接口隔离熔断

优惠券内部依赖了第三方的系统，为了防止因为依赖方服务不可用，产生连锁效应，最终导致优惠券服务雪崩的事情发生，优惠券对依赖外部接口做了隔离和熔断。

用户维度优惠券字段冗余

查询用户相关的优惠券数据是优惠券最频繁的查询操作之一，用户优惠券数据做了分库分表，在查询时无法关联券规则表进行查询，为了减少IO次数，用户优惠券表中冗余了部分券规则的字段。优惠券规则表字段较多，冗余的字段不能很多，要在性能和字段数之间做好平衡。


7、优惠券架构是如何演化   https://zhuanlan.zhihu.com/p/544294494

8、Disruptor在云音乐特征服务中的应用   https://juejin.cn/post/7096382878761615374  https://jitwxs.cn/17fac167

线上特征数据服务DataService，为了解决使用线程池模型导致机器cpu利用率不高，长尾请求延迟不线性（p99、p999出现J型曲线）的问题。在利用Disruptor替换线程池之后取得不错的性能结果

Disruptor 的设计初衷就是为了解决上面提到的问题【注：锁代价过大、CAS代价过大、缓存行导致的伪共享、队列存在的问题】，以求最优化内存分配，使用缓存友好的方式来最佳使用现代硬件资源。

研发的初衷是解决内存队列的延迟问题，disruptor一般用于线程间消息的传递。

disruptor是用于一个JVM中多个线程之间的消息队列，作用与ArrayBlockingQueue有相似之处，但是disruptor从功能、性能都远好于ArrayBlockingQueue，当多个线程之间传递大量数据或对性能要求较高时，可以考虑使用disruptor作为ArrayBlockingQueue的替代者。


不管是单生产者、还是多生产者数据的消费都是不受影响的。Disruptor支持开启多个Processor（也就是线程），每个Processor使用类似while true的模式拉取可消费的事件进行处理。
这样的跟线程池模式的好处是避免线程创建、销毁、上下文切换代理的性能损失（缓存污染……）


测试Case
通过异步客户端访问特征服务随机查询若干特征，特征存储在（Redis、Tair、Hbase）三种外部存储中
测试线程池、Disruptor两个处理队列的吞吐能力、响应延迟分布



技术框架
SpringBoot + Mysql + Redis + Hive + RocketMq + Dubbo + Nacos + Sentinel + Elasticsearch
负责模块
主要负责开发、设计、优化秒杀活动、抽奖活动、优惠券、司机权益等营销活动，提高系统并发量和稳定性 

秒杀活动项目亮点
1、服务优化：通过构建多级缓存、异步、削峰、优化限购、限流策略等方式提高服务性能，峰值QPS达到20000左右、RT提升50%以上
2、库存扣减优化：使用Redis Map结构，通过库存预锁定字段来优化链路正向和逆向流程
3、服务毛刺优化：优化服务的负载均衡策略，解决接口响应时间波动和毛刺，平均响应时间降低约10% 
4、Dubbo连接超时：排查线上Dubbo连接超时问题，通过测试调整backlog大小，避免syn丢包

优惠券系统项目亮点
1、	库存扣减和接口优化：高并发情况下通过优化库存扣减方式、分布式锁的获取方式、接口批量处理和幂等
等方式提高接口性能
2、解决数据库压力问题：通过Caffeine和Redis构建多级缓存降低数据库压力，实现缓存数据的命中率达到了80% 以上
3、线程优化：使用Disruptor代替原先jdk的线程池，提高接口响应时间，降低超时率
4、gc优化：成功优化服务gc耗时和频率，将YGC优化至20ms左右，mixed GC优化至10-40ms左右

技术框架
Golang
负责模块 
实现四个实验，包括：MapReduce函数、Raft协议、KV数据库、数据库Sharding等功能
项目亮点
1、实现完整的Raft协议，包括Leader Election、AppendEntry RPC 和 Raft persist 
2、在Raft基础上，优化其日志压缩(log compaction)和加速日志回溯(log backtracking)部分
3、基于Raft库构建容错的Key/Value服务，实现 Raft 协议的 Snapshot RPC，线性化语义，即保证日志仅被
执行一次，同时实现Sharding功能，即 multi Raft
4、优化Sharding功能，实现高可用的集群配置管理服务。其用来记录每个Raft组对应的多个节点的 endpoint 
以及shard的分配信息

技术框架
C++
负责模块
实现单机关系型数据库，包括: Buffer Pool缓冲池、可扩展性哈希表、SQL语句执行、事务并发控制等
项目亮点
1、实现Buffer Pool缓冲池管理，基于 LRU 算法淘汰策略，支持可扩展哈希表，即表容量动态扩容和收缩，
该哈希表支持磁盘存储，负责快速搜索，而不必搜索数据库表中的每一条记录 
2、语句执行采用火山模型原理实现，支持基本SELECT，DELETE，UPDATE，JOIN，AGGREGATION等操作。Join操作包括有 Nested Loop Join和Hash Join两种实现
3、实现锁管理器Lock Manager，支持并发执行，并发控制采用 2PL 设计，支持 RU, RC, RR 3种隔离级别
4、锁管理器支持死锁预防，使用WOUND-WAIT算法来决定中止哪些事务

















































































































































































































































































































































































































































